{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fatal-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"spark.kubernetes.namespace\": \"spark\",\n",
    "    \"spark.kubernetes.container.image\": \"itayb/spark:3.1.1-hadoop-3.2.0-aws\",\n",
    "    \"spark.executor.instances\": \"2\",\n",
    "    \"spark.executor.memory\": \"4g\",\n",
    "    \"spark.executor.cores\": \"1\",\n",
    "    \"spark.driver.blockManager.port\": \"7777\",\n",
    "    \"spark.driver.port\": \"2222\",\n",
    "    \"spark.driver.host\": \"jupyter.spark.svc.cluster.local\",\n",
    "    \"spark.driver.bindAddress\": \"0.0.0.0\",\n",
    "    \"spark.hadoop.fs.s3a.endpoint\": \"localstack.kube-system.svc.cluster.local:4566\",\n",
    "    \"spark.hadoop.fs.s3a.connection.ssl.enabled\": \"false\",\n",
    "    \"spark.hadoop.fs.s3a.path.style.access\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.impl\": \"org.apache.hadoop.fs.s3a.S3AFileSystem\",\n",
    "    \"spark.hadoop.com.amazonaws.services.s3.enableV4\": \"true\",\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\": \"org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\",\n",
    "}\n",
    "\n",
    "def get_spark_session(app_name: str, conf: SparkConf):\n",
    "    conf.setMaster(\"k8s://https://kubernetes.default.svc.cluster.local\")\n",
    "    for key, value in config.items():\n",
    "        conf.set(key, value)    \n",
    "    return SparkSession.builder.appName(app_name).config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "successful-retirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark_session(\"spark-workers\", swan_spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "velvet-harvard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- course_title: string (nullable = true)\n",
      " |-- course_organization: string (nullable = true)\n",
      " |-- course_Certificate_type: string (nullable = true)\n",
      " |-- course_rating: string (nullable = true)\n",
      " |-- course_difficulty: string (nullable = true)\n",
      " |-- course_students_enrolled: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "course_df = spark.read.csv('s3a://bds-assignment/data/coursera_data.csv',header=True)\n",
    "course_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "effective-czech",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- jobpost: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- Title: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- AnnouncementCode: string (nullable = true)\n",
      " |-- Term: string (nullable = true)\n",
      " |-- Eligibility: string (nullable = true)\n",
      " |-- Audience: string (nullable = true)\n",
      " |-- StartDate: string (nullable = true)\n",
      " |-- Duration: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- JobDescription: string (nullable = true)\n",
      " |-- JobRequirment: string (nullable = true)\n",
      " |-- RequiredQual: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      " |-- ApplicationP: string (nullable = true)\n",
      " |-- OpeningDate: string (nullable = true)\n",
      " |-- Deadline: string (nullable = true)\n",
      " |-- Notes: string (nullable = true)\n",
      " |-- AboutC: string (nullable = true)\n",
      " |-- Attach: string (nullable = true)\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- IT: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jobs_df = spark.read.csv('s3a://bds-assignment/data/data_job_post.csv',header=True)\n",
    "jobs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "characteristic-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_df_subset = course_df.select(\"course_title\")\n",
    "jobs_df_subset = jobs_df.select(\"jobpost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "italic-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_df_subset = jobs_df_subset.na.drop()\n",
    "course_df_subset = course_df_subset.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "planned-mainland",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_df_subset = course_df_subset.toDF(*[\"text\"])\n",
    "jobs_df_subset = jobs_df_subset.toDF(*[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "minus-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "course_df_subset = course_df_subset.select('text').withColumn(\"doc_id\", F.expr(\"uuid()\"))\n",
    "jobs_df_subset = jobs_df_subset.select('text').withColumn(\"doc_id\", F.expr(\"uuid()\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "equipped-cycling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                text|              doc_id|\n",
      "+--------------------+--------------------+\n",
      "|AMERIA Investment...|1cbf5725-d6de-414...|\n",
      "|JOB TITLE:  Chief...|f1f47a8f-4d51-4bd...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jobs_df_subset.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "furnished-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "varying-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['text'] \n",
    "\n",
    "\n",
    "preProcStages = []\n",
    "\n",
    "for col in columns:\n",
    "    regexTokenizer = RegexTokenizer(gaps=False, pattern='\\w+', inputCol=col, outputCol=col+'Token')\n",
    "    stopWordsRemover = StopWordsRemover(inputCol=col+'Token', outputCol=col+'SWRemoved')\n",
    "    countVectorizer = CountVectorizer(inputCol=col+'SWRemoved', outputCol=col+'TF')\n",
    "    idf = IDF(inputCol=col+'TF', outputCol=col+'IDF') \n",
    "    preProcStages += [regexTokenizer, stopWordsRemover, countVectorizer, idf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "valid-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=preProcStages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "educated-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine two dataframes\n",
    "dataCombined = course_df_subset.union(jobs_df_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "forward-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCombined = dataCombined.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "wanted-devon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[text: string, doc_id: string]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataCombined.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "chicken-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(dataCombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "three-today",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataCombined = model.transform(dataCombined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "necessary-somalia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|              doc_id|           textToken|       textSWRemoved|              textTF|             textIDF|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|Financing and Inv...|c93c686b-8fb1-492...|[financing, and, ...|[financing, inves...|(787,[589,599,692...|(787,[589,599,692...|\n",
      "|Epidemiology: The...|e00b050c-1dbf-43e...|[epidemiology, th...|[epidemiology, ba...|(787,[40,43,113,7...|(787,[40,43,113,7...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataCombined.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "alpha-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookupTable = spark.sparkContext.broadcast(dataCombined.rdd.map(lambda x: (x['doc_id'], \n",
    "                                                           {'text':x['text'], \n",
    "                                                            'textIDF':x['textIDF']})).collectAsMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "armed-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSINE similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "sonic-island",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def cosine_similarity(X, Y):\n",
    "    denom = X.norm(2) * Y.norm(2)\n",
    "    if denom == 0.0:\n",
    "        return float(-1.0)\n",
    "    else:\n",
    "        return float(X.dot(Y) / float(denom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "closed-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarities(id_course, id_job, lookupTable):\n",
    "    X, Y = lookupTable.value[id_course], lookupTable.value[id_job]\n",
    "  \n",
    "    sim = cosine_similarity(X['textIDF'], Y['textIDF'])\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "operational-prefix",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "pairId = jobs_df_subset.select('doc_id').rdd.flatMap(list).cartesian(jobs_df_subset.select('doc_id').rdd.flatMap(list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "accomplished-ontario",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce'),\n",
       " ('1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  'f1f47a8f-4d51-4bd2-b4cf-5278e30f3ad7')]"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairId.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "smooth-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairProdDF = pairId.map(lambda x : (x[0],x[1],  similarities(x[0], x[1], lookupTable)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "white-drain",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  1.0),\n",
       " ('1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  'f1f47a8f-4d51-4bd2-b4cf-5278e30f3ad7',\n",
       "  0.0),\n",
       " ('1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  '8c34a43e-306a-4c18-848c-4bce89fbf94f',\n",
       "  0.0),\n",
       " ('1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  'b649b6f8-9b46-4162-a55a-0e70d74267e5',\n",
       "  0.7371461382089741),\n",
       " ('1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  'e7e14178-ba4c-4600-b77a-860aa7f4d648',\n",
       "  0.11476631991613744)]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairProdDF.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "processed-destruction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(doc_id='1cbf5725-d6de-414d-9c9d-438c10bdd6ce')]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_df_subset.limit(1).select(\"doc_id\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "exotic-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultDF = pairProdDF.filter(lambda x : x[1] == \"1cbf5725-d6de-414d-9c9d-438c10bdd6ce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "pretty-translator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  1.0),\n",
       " ('f1f47a8f-4d51-4bd2-b4cf-5278e30f3ad7',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.0),\n",
       " ('8c34a43e-306a-4c18-848c-4bce89fbf94f',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.0),\n",
       " ('b649b6f8-9b46-4162-a55a-0e70d74267e5',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.7371461382089741),\n",
       " ('e7e14178-ba4c-4600-b77a-860aa7f4d648',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.11476631991613744),\n",
       " ('42936415-eeb5-474e-aa68-35c061e03b61',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.0),\n",
       " ('4d5cb423-bb20-4bf4-a5fc-0b9aa5948c3c',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.0),\n",
       " ('e0d1c970-1bef-4d96-a1bd-a40dc98a40cf',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.0),\n",
       " ('60e60a75-e7a6-40a4-aafd-4cbe9842ce36',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.4047057683806313),\n",
       " ('3c8b32c1-9b08-416a-be62-d098db67d7cb',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.0)]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultDF.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "sophisticated-receptor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[550] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "rolled-charity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  1.0),\n",
       " ('b649b6f8-9b46-4162-a55a-0e70d74267e5',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.7371461382089741),\n",
       " ('60e60a75-e7a6-40a4-aafd-4cbe9842ce36',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.4047057683806313),\n",
       " ('e1a83059-ad89-4f05-a260-1cff029a6be7',\n",
       "  '1cbf5725-d6de-414d-9c9d-438c10bdd6ce',\n",
       "  0.4047057683806313)]"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultDF.sortBy(lambda x : x[2],ascending = False).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-construction",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
